{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen \n",
    "from urllib.error import HTTPError \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.error import URLError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import requests\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import json\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from utils import save_as_json, load_json, load_page, extract_links, clean_title \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(url, query, dir, filename):\n",
    "    \"\"\"\n",
    "    Crawl the webpage to extract links\n",
    "    :param url: URL of the webpage\n",
    "    :param query: query to search\n",
    "    :param dir: directory to save the links\n",
    "    :param filename: name of the file\n",
    "    \"\"\"\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    links_to_scrape = []\n",
    "    try:\n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "\n",
    "        # Dropdown menu options\n",
    "        time_period = ['2014 - 2019', '2009 - 2014', '2004 - 2009', '1999 - 2004', '2019 - 2024']\n",
    "        \n",
    "        for period in time_period:\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Extracting links from {period}\")\n",
    "            # Find and click on the dropdown button to open the dropdown menu\n",
    "            dropdown = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '#criteriaSidesLeg-button')))\n",
    "            dropdown.click()\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Locate the option in the dropdown menu and click on it\n",
    "            option = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, f'//ul[@id=\"criteriaSidesLeg-menu\"]/li/a[contains(text(), \"{period}\")]')))\n",
    "            option.location_once_scrolled_into_view\n",
    "            option.click()\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Find the search input field and insert the query\n",
    "            search_input = driver.find_element(By.CSS_SELECTOR, '#criteriaSidesMiText.field_full_width')\n",
    "            search_input.clear()  # Clear any previous input\n",
    "            search_input.send_keys(query)\n",
    "            time.sleep(3)\n",
    "            print(\"Query inserted\")\n",
    "\n",
    "            search_button = driver.find_element(By.ID, 'sidesButtonSubmit')\n",
    "            search_button.click()\n",
    "            print(\"Search button clicked\")\n",
    "            time.sleep(5)\n",
    "\n",
    "            # Get the HTML content of the page after search\n",
    "            html_content = driver.page_source\n",
    "\n",
    "            # Extract links from the HTML content\n",
    "            extracted_links = extract_links(html_content)\n",
    "            links_to_scrape.extend(extracted_links)\n",
    "\n",
    "            # Check if there is a next page\n",
    "            try:\n",
    "                first_page = driver.find_element(By.XPATH, '//*[@id=\"content_left\"]/div[2]/div[15]/a')\n",
    "            except NoSuchElementException:\n",
    "                first_page = None\n",
    "            \n",
    "            if first_page:\n",
    "                print(\"Next page found.\")\n",
    "                html_content = driver.page_source\n",
    "\n",
    "                # Click on the next page\n",
    "                first_page.location_once_scrolled_into_view\n",
    "                first_page.click()\n",
    "                time.sleep(3)\n",
    "\n",
    "                # Extract links from the next page\n",
    "                links_to_scrape.extend(extract_links(html_content))\n",
    "\n",
    "            # Check if there is a next button on following pages after the first one\n",
    "            try:\n",
    "                next_button = driver.find_element(By.XPATH, '//*[@id=\"content_left\"]/div[2]/div[15]/a[2]')\n",
    "            except NoSuchElementException:\n",
    "                next_button = None\n",
    "\n",
    "            while next_button:\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"content_left\"]/div[2]/div[15]/a[2]')))\n",
    "                next_button.location_once_scrolled_into_view\n",
    "                next_button.click()\n",
    "                time.sleep(3)\n",
    "\n",
    "                html_content = driver.page_source\n",
    "                links_to_scrape.extend(extract_links(html_content))\n",
    "                print(\"Links extracted.\")\n",
    "\n",
    "                print(\"Proceed to the next page.\")\n",
    "                time.sleep(3)\n",
    "\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.XPATH, '//*[@id=\"content_left\"]/div[2]/div[15]/a[2]')\n",
    "                except NoSuchElementException:\n",
    "                    break\n",
    "\n",
    "            print(\"Links extracted from the last page.\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        save_as_json(links_to_scrape, dir, f\"{filename}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the user to enter the URL of the webpage\n",
    "url = input(\"Enter the URL of the webpage: \")\n",
    "# Prompt the user to enter the query\n",
    "query = input(\"Enter the query: \")\n",
    "# Prompt the user to enter the directory to save the links\n",
    "dir = input(\"Enter the directory to save the links: \")\n",
    "# Prompt the user to enter the filename\n",
    "filename = input(\"Enter the filename without any extention: \")\n",
    "\n",
    "crawler(url, query, dir, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pages from the JSON file \n",
    "links = load_json('..\\\\json_files\\\\links.json')\n",
    "\n",
    "def extract_role_from_paragraph(paragraph):\n",
    "    italic_span = paragraph.find('span', {'class': 'italic'})\n",
    "    \n",
    "    if italic_span:\n",
    "        return italic_span.get_text(strip=True)\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(urls, filename):\n",
    "    all_extracted_content = []\n",
    "\n",
    "    for url in urls:\n",
    "        soup = load_page(url)\n",
    "\n",
    "        if soup is None:\n",
    "            print(f\"The page {url} does not exist\")\n",
    "            continue\n",
    "\n",
    "        extracted_content = {}\n",
    "        date = soup.find('td', class_='doc_title').get_text(strip=True)\n",
    "        title = soup.find('td', class_='doc_title', style=\"background-image:url(/doceo/data/img/gradient_blue.gif)\").get_text(strip=True)\n",
    "        filtered_title = clean_title(title)\n",
    "\n",
    "        extracted_content[\"title\"] = filtered_title\n",
    "        extracted_content[\"date\"] = date\n",
    "        extracted_content[\"link\"] = url\n",
    "        extracted_content[\"speakers\"] = {}\n",
    "\n",
    "        for speaker_tag in soup.findAll('p', {'class': 'contents'}):\n",
    "            speaker_name_span = speaker_tag.find('span', {'class': 'doc_subtitle_level1_bis'})\n",
    "            if speaker_name_span:\n",
    "                speaker_name = speaker_name_span.get_text(strip=True)\n",
    "                speaker_role = extract_role_from_paragraph(speaker_tag)\n",
    "                speaker_name = f\"{speaker_name} ({speaker_role})\" if speaker_role else speaker_name\n",
    "\n",
    "                if speaker_name not in extracted_content[\"speakers\"]:\n",
    "                    extracted_content[\"speakers\"][speaker_name] = {\"text\": \"\"}\n",
    "                \n",
    "                # Directly append the text from the <p class=\"contents\"> element\n",
    "                text = speaker_tag.get_text(strip=True)\n",
    "                extracted_content[\"speakers\"][speaker_name][\"text\"] += text\n",
    "\n",
    "        all_extracted_content.append(extracted_content)\n",
    "\n",
    "    with open(f\"{filename}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        fieldnames = [\"title\", \"date\", \"speaker\", \"text\", \"link\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for content in all_extracted_content:\n",
    "            title = content[\"title\"]\n",
    "            date = content[\"date\"]\n",
    "            url = content[\"link\"]\n",
    "            for speaker, details in content[\"speakers\"].items():\n",
    "                writer.writerow({\n",
    "                    \"title\": title,\n",
    "                    \"date\": date,\n",
    "                    \"speaker\": speaker,\n",
    "                    \"text\": details[\"text\"],\n",
    "                    \"link\": url\n",
    "                })\n",
    "\n",
    "    print(\"CSV file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page https://www.europarl.europa.eu/doceo/document/CRE-8-`2`018-09-11-ITM-015_EN.html does not exist\n",
      "CSV file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "scrape_data(links, \"data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
